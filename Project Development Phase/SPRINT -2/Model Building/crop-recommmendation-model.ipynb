{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df = pd.read_csv('../Data-processed/crop-recommendation.csv')\n",
    "df.head()\n",
    "N\tP\tK\ttemperature\thumidity\tph\trainfall\tlabel\n",
    "0\t90\t42\t43\t20.879744\t82.002744\t6.502985\t202.935536\trice\n",
    "1\t85\t58\t41\t21.770462\t80.319644\t7.038096\t226.655537\trice\n",
    "2\t60\t55\t44\t23.004459\t82.320763\t7.840207\t263.964248\trice\n",
    "3\t74\t35\t40\t26.491096\t80.158363\t6.980401\t242.864034\trice\n",
    "4\t78\t42\t42\t20.130175\t81.604873\t7.628473\t262.717340\trice\n",
    "df.tail()\n",
    "N\tP\tK\ttemperature\thumidity\tph\trainfall\tlabel\n",
    "2195\t107\t34\t32\t26.774637\t66.413269\t6.780064\t177.774507\tcoffee\n",
    "2196\t99\t15\t27\t27.417112\t56.636362\t6.086922\t127.924610\tcoffee\n",
    "2197\t118\t33\t30\t24.131797\t67.225123\t6.362608\t173.322839\tcoffee\n",
    "2198\t117\t32\t34\t26.272418\t52.127394\t6.758793\t127.175293\tcoffee\n",
    "2199\t104\t18\t30\t23.603016\t60.396475\t6.779833\t140.937041\tcoffee\n",
    "df.size\n",
    "17600\n",
    "df.shape\n",
    "(2200, 8)\n",
    "df.columns\n",
    "Index(['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall', 'label'], dtype='object')\n",
    "df['label'].unique()\n",
    "array(['rice', 'maize', 'chickpea', 'kidneybeans', 'pigeonpeas',\n",
    "       'mothbeans', 'mungbean', 'blackgram', 'lentil', 'pomegranate',\n",
    "       'banana', 'mango', 'grapes', 'watermelon', 'muskmelon', 'apple',\n",
    "       'orange', 'papaya', 'coconut', 'cotton', 'jute', 'coffee'],\n",
    "      dtype=object)\n",
    "df.dtypes\n",
    "N                int64\n",
    "P                int64\n",
    "K                int64\n",
    "temperature    float64\n",
    "humidity       float64\n",
    "ph             float64\n",
    "rainfall       float64\n",
    "label           object\n",
    "dtype: object\n",
    "df['label'].value_counts()\n",
    "muskmelon      100\n",
    "kidneybeans    100\n",
    "papaya         100\n",
    "pigeonpeas     100\n",
    "blackgram      100\n",
    "cotton         100\n",
    "mothbeans      100\n",
    "mungbean       100\n",
    "watermelon     100\n",
    "orange         100\n",
    "mango          100\n",
    "banana         100\n",
    "rice           100\n",
    "pomegranate    100\n",
    "chickpea       100\n",
    "apple          100\n",
    "jute           100\n",
    "grapes         100\n",
    "lentil         100\n",
    "coffee         100\n",
    "maize          100\n",
    "coconut        100\n",
    "Name: label, dtype: int64\n",
    "sns.heatmap(df.corr(),annot=True)\n",
    "\n",
    "Seperating features and target label\n",
    "features = df[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]\n",
    "target = df['label']\n",
    "#features = df[['temperature', 'humidity', 'ph', 'rainfall']]\n",
    "labels = df['label']\n",
    "# Initialzing empty lists to append all model's name and corresponding name\n",
    "acc = []\n",
    "model = []\n",
    "# Splitting into train and test data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.2,random_state =2)\n",
    "Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DecisionTree = DecisionTreeClassifier(criterion=\"entropy\",random_state=2,max_depth=5)\n",
    "\n",
    "DecisionTree.fit(Xtrain,Ytrain)\n",
    "\n",
    "predicted_values = DecisionTree.predict(Xtest)\n",
    "x = metrics.accuracy_score(Ytest, predicted_values)\n",
    "acc.append(x)\n",
    "model.append('Decision Tree')\n",
    "print(\"DecisionTrees's Accuracy is: \", x*100)\n",
    "\n",
    "print(classification_report(Ytest,predicted_values))\n",
    "DecisionTrees's Accuracy is:  90.0\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       apple       1.00      1.00      1.00        13\n",
    "      banana       1.00      1.00      1.00        17\n",
    "   blackgram       0.59      1.00      0.74        16\n",
    "    chickpea       1.00      1.00      1.00        21\n",
    "     coconut       0.91      1.00      0.95        21\n",
    "      coffee       1.00      1.00      1.00        22\n",
    "      cotton       1.00      1.00      1.00        20\n",
    "      grapes       1.00      1.00      1.00        18\n",
    "        jute       0.74      0.93      0.83        28\n",
    " kidneybeans       0.00      0.00      0.00        14\n",
    "      lentil       0.68      1.00      0.81        23\n",
    "       maize       1.00      1.00      1.00        21\n",
    "       mango       1.00      1.00      1.00        26\n",
    "   mothbeans       0.00      0.00      0.00        19\n",
    "    mungbean       1.00      1.00      1.00        24\n",
    "   muskmelon       1.00      1.00      1.00        23\n",
    "      orange       1.00      1.00      1.00        29\n",
    "      papaya       1.00      0.84      0.91        19\n",
    "  pigeonpeas       0.62      1.00      0.77        18\n",
    " pomegranate       1.00      1.00      1.00        17\n",
    "        rice       1.00      0.62      0.77        16\n",
    "  watermelon       1.00      1.00      1.00        15\n",
    "\n",
    "    accuracy                           0.90       440\n",
    "   macro avg       0.84      0.88      0.85       440\n",
    "weighted avg       0.86      0.90      0.87       440\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Cross validation score (Decision Tree)\n",
    "score = cross_val_score(DecisionTree, features, target,cv=5)\n",
    "score\n",
    "array([0.93636364, 0.90909091, 0.91818182, 0.87045455, 0.93636364])\n",
    "Saving trained Decision Tree model\n",
    "import pickle\n",
    "# Dump the trained Naive Bayes classifier with Pickle\n",
    "DT_pkl_filename = '../models/DecisionTree.pkl'\n",
    "# Open the file to save as pkl file\n",
    "DT_Model_pkl = open(DT_pkl_filename, 'wb')\n",
    "pickle.dump(DecisionTree, DT_Model_pkl)\n",
    "# Close the pickle instances\n",
    "DT_Model_pkl.close()\n",
    "Guassian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NaiveBayes = GaussianNB()\n",
    "\n",
    "NaiveBayes.fit(Xtrain,Ytrain)\n",
    "\n",
    "predicted_values = NaiveBayes.predict(Xtest)\n",
    "x = metrics.accuracy_score(Ytest, predicted_values)\n",
    "acc.append(x)\n",
    "model.append('Naive Bayes')\n",
    "print(\"Naive Bayes's Accuracy is: \", x)\n",
    "\n",
    "print(classification_report(Ytest,predicted_values))\n",
    "Naive Bayes's Accuracy is:  0.990909090909091\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       apple       1.00      1.00      1.00        13\n",
    "      banana       1.00      1.00      1.00        17\n",
    "   blackgram       1.00      1.00      1.00        16\n",
    "    chickpea       1.00      1.00      1.00        21\n",
    "     coconut       1.00      1.00      1.00        21\n",
    "      coffee       1.00      1.00      1.00        22\n",
    "      cotton       1.00      1.00      1.00        20\n",
    "      grapes       1.00      1.00      1.00        18\n",
    "        jute       0.88      1.00      0.93        28\n",
    " kidneybeans       1.00      1.00      1.00        14\n",
    "      lentil       1.00      1.00      1.00        23\n",
    "       maize       1.00      1.00      1.00        21\n",
    "       mango       1.00      1.00      1.00        26\n",
    "   mothbeans       1.00      1.00      1.00        19\n",
    "    mungbean       1.00      1.00      1.00        24\n",
    "   muskmelon       1.00      1.00      1.00        23\n",
    "      orange       1.00      1.00      1.00        29\n",
    "      papaya       1.00      1.00      1.00        19\n",
    "  pigeonpeas       1.00      1.00      1.00        18\n",
    " pomegranate       1.00      1.00      1.00        17\n",
    "        rice       1.00      0.75      0.86        16\n",
    "  watermelon       1.00      1.00      1.00        15\n",
    "\n",
    "    accuracy                           0.99       440\n",
    "   macro avg       0.99      0.99      0.99       440\n",
    "weighted avg       0.99      0.99      0.99       440\n",
    "\n",
    "# Cross validation score (NaiveBayes)\n",
    "score = cross_val_score(NaiveBayes,features,target,cv=5)\n",
    "score\n",
    "array([0.99772727, 0.99545455, 0.99545455, 0.99545455, 0.99090909])\n",
    "Saving trained Guassian Naive Bayes model\n",
    "import pickle\n",
    "# Dump the trained Naive Bayes classifier with Pickle\n",
    "NB_pkl_filename = '../models/NBClassifier.pkl'\n",
    "# Open the file to save as pkl file\n",
    "NB_Model_pkl = open(NB_pkl_filename, 'wb')\n",
    "pickle.dump(NaiveBayes, NB_Model_pkl)\n",
    "# Close the pickle instances\n",
    "NB_Model_pkl.close()\n",
    "Support Vector Machine (SVM)\n",
    "from sklearn.svm import SVC\n",
    "# data normalization with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(Xtrain)\n",
    "X_train_norm = norm.transform(Xtrain)\n",
    "# transform testing dataabs\n",
    "X_test_norm = norm.transform(Xtest)\n",
    "SVM = SVC(kernel='poly', degree=3, C=1)\n",
    "SVM.fit(X_train_norm,Ytrain)\n",
    "predicted_values = SVM.predict(X_test_norm)\n",
    "x = metrics.accuracy_score(Ytest, predicted_values)\n",
    "acc.append(x)\n",
    "model.append('SVM')\n",
    "print(\"SVM's Accuracy is: \", x)\n",
    "\n",
    "print(classification_report(Ytest,predicted_values))\n",
    "SVM's Accuracy is:  0.9795454545454545\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       apple       1.00      1.00      1.00        13\n",
    "      banana       1.00      1.00      1.00        17\n",
    "   blackgram       1.00      1.00      1.00        16\n",
    "    chickpea       1.00      1.00      1.00        21\n",
    "     coconut       1.00      1.00      1.00        21\n",
    "      coffee       1.00      0.95      0.98        22\n",
    "      cotton       0.95      1.00      0.98        20\n",
    "      grapes       1.00      1.00      1.00        18\n",
    "        jute       0.83      0.89      0.86        28\n",
    " kidneybeans       1.00      1.00      1.00        14\n",
    "      lentil       1.00      1.00      1.00        23\n",
    "       maize       1.00      0.95      0.98        21\n",
    "       mango       1.00      1.00      1.00        26\n",
    "   mothbeans       1.00      1.00      1.00        19\n",
    "    mungbean       1.00      1.00      1.00        24\n",
    "   muskmelon       1.00      1.00      1.00        23\n",
    "      orange       1.00      1.00      1.00        29\n",
    "      papaya       1.00      1.00      1.00        19\n",
    "  pigeonpeas       1.00      1.00      1.00        18\n",
    " pomegranate       1.00      1.00      1.00        17\n",
    "        rice       0.80      0.75      0.77        16\n",
    "  watermelon       1.00      1.00      1.00        15\n",
    "\n",
    "    accuracy                           0.98       440\n",
    "   macro avg       0.98      0.98      0.98       440\n",
    "weighted avg       0.98      0.98      0.98       440\n",
    "\n",
    "# Cross validation score (SVM)\n",
    "score = cross_val_score(SVM,features,target,cv=5)\n",
    "score\n",
    "array([0.97954545, 0.975     , 0.98863636, 0.98863636, 0.98181818])\n",
    "#Saving trained SVM model\n",
    "import pickle\n",
    "# Dump the trained SVM classifier with Pickle\n",
    "SVM_pkl_filename = '../models/SVMClassifier.pkl'\n",
    "# Open the file to save as pkl file\n",
    "SVM_Model_pkl = open(SVM_pkl_filename, 'wb')\n",
    "pickle.dump(SVM, SVM_Model_pkl)\n",
    "# Close the pickle instances\n",
    "SVM_Model_pkl.close()\n",
    "Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogReg = LogisticRegression(random_state=2)\n",
    "\n",
    "LogReg.fit(Xtrain,Ytrain)\n",
    "\n",
    "predicted_values = LogReg.predict(Xtest)\n",
    "\n",
    "x = metrics.accuracy_score(Ytest, predicted_values)\n",
    "acc.append(x)\n",
    "model.append('Logistic Regression')\n",
    "print(\"Logistic Regression's Accuracy is: \", x)\n",
    "\n",
    "print(classification_report(Ytest,predicted_values))\n",
    "Logistic Regression's Accuracy is:  0.9522727272727273\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       apple       1.00      1.00      1.00        13\n",
    "      banana       1.00      1.00      1.00        17\n",
    "   blackgram       0.86      0.75      0.80        16\n",
    "    chickpea       1.00      1.00      1.00        21\n",
    "     coconut       1.00      1.00      1.00        21\n",
    "      coffee       1.00      1.00      1.00        22\n",
    "      cotton       0.86      0.90      0.88        20\n",
    "      grapes       1.00      1.00      1.00        18\n",
    "        jute       0.84      0.93      0.88        28\n",
    " kidneybeans       1.00      1.00      1.00        14\n",
    "      lentil       0.88      1.00      0.94        23\n",
    "       maize       0.90      0.86      0.88        21\n",
    "       mango       0.96      1.00      0.98        26\n",
    "   mothbeans       0.84      0.84      0.84        19\n",
    "    mungbean       1.00      0.96      0.98        24\n",
    "   muskmelon       1.00      1.00      1.00        23\n",
    "      orange       1.00      1.00      1.00        29\n",
    "      papaya       1.00      0.95      0.97        19\n",
    "  pigeonpeas       1.00      1.00      1.00        18\n",
    " pomegranate       1.00      1.00      1.00        17\n",
    "        rice       0.85      0.69      0.76        16\n",
    "  watermelon       1.00      1.00      1.00        15\n",
    "\n",
    "    accuracy                           0.95       440\n",
    "   macro avg       0.95      0.95      0.95       440\n",
    "weighted avg       0.95      0.95      0.95       440\n",
    "\n",
    "# Cross validation score (Logistic Regression)\n",
    "score = cross_val_score(LogReg,features,target,cv=5)\n",
    "score\n",
    "array([0.95      , 0.96590909, 0.94772727, 0.96590909, 0.94318182])\n",
    "Saving trained Logistic Regression model\n",
    "import pickle\n",
    "# Dump the trained Naive Bayes classifier with Pickle\n",
    "LR_pkl_filename = '../models/LogisticRegression.pkl'\n",
    "# Open the file to save as pkl file\n",
    "LR_Model_pkl = open(DT_pkl_filename, 'wb')\n",
    "pickle.dump(LogReg, LR_Model_pkl)\n",
    "# Close the pickle instances\n",
    "LR_Model_pkl.close()\n",
    "Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "RF.fit(Xtrain,Ytrain)\n",
    "\n",
    "predicted_values = RF.predict(Xtest)\n",
    "\n",
    "x = metrics.accuracy_score(Ytest, predicted_values)\n",
    "acc.append(x)\n",
    "model.append('RF')\n",
    "print(\"RF's Accuracy is: \", x)\n",
    "\n",
    "print(classification_report(Ytest,predicted_values))\n",
    "RF's Accuracy is:  0.990909090909091\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       apple       1.00      1.00      1.00        13\n",
    "      banana       1.00      1.00      1.00        17\n",
    "   blackgram       0.94      1.00      0.97        16\n",
    "    chickpea       1.00      1.00      1.00        21\n",
    "     coconut       1.00      1.00      1.00        21\n",
    "      coffee       1.00      1.00      1.00        22\n",
    "      cotton       1.00      1.00      1.00        20\n",
    "      grapes       1.00      1.00      1.00        18\n",
    "        jute       0.90      1.00      0.95        28\n",
    " kidneybeans       1.00      1.00      1.00        14\n",
    "      lentil       1.00      1.00      1.00        23\n",
    "       maize       1.00      1.00      1.00        21\n",
    "       mango       1.00      1.00      1.00        26\n",
    "   mothbeans       1.00      0.95      0.97        19\n",
    "    mungbean       1.00      1.00      1.00        24\n",
    "   muskmelon       1.00      1.00      1.00        23\n",
    "      orange       1.00      1.00      1.00        29\n",
    "      papaya       1.00      1.00      1.00        19\n",
    "  pigeonpeas       1.00      1.00      1.00        18\n",
    " pomegranate       1.00      1.00      1.00        17\n",
    "        rice       1.00      0.81      0.90        16\n",
    "  watermelon       1.00      1.00      1.00        15\n",
    "\n",
    "    accuracy                           0.99       440\n",
    "   macro avg       0.99      0.99      0.99       440\n",
    "weighted avg       0.99      0.99      0.99       440\n",
    "\n",
    "# Cross validation score (Random Forest)\n",
    "score = cross_val_score(RF,features,target,cv=5)\n",
    "score\n",
    "array([0.99772727, 0.99545455, 0.99772727, 0.99318182, 0.98863636])\n",
    "Saving trained Random Forest model\n",
    "import pickle\n",
    "# Dump the trained Naive Bayes classifier with Pickle\n",
    "RF_pkl_filename = '../models/RandomForest.pkl'\n",
    "# Open the file to save as pkl file\n",
    "RF_Model_pkl = open(RF_pkl_filename, 'wb')\n",
    "pickle.dump(RF, RF_Model_pkl)\n",
    "# Close the pickle instances\n",
    "RF_Model_pkl.close()\n",
    "XGBoost\n",
    "import xgboost as xgb\n",
    "XB = xgb.XGBClassifier()\n",
    "XB.fit(Xtrain,Ytrain)\n",
    "\n",
    "predicted_values = XB.predict(Xtest)\n",
    "\n",
    "x = metrics.accuracy_score(Ytest, predicted_values)\n",
    "acc.append(x)\n",
    "model.append('XGBoost')\n",
    "print(\"XGBoost's Accuracy is: \", x)\n",
    "\n",
    "print(classification_report(Ytest,predicted_values))\n",
    "[14:16:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "XGBoost's Accuracy is:  0.9931818181818182\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       apple       1.00      1.00      1.00        13\n",
    "      banana       1.00      1.00      1.00        17\n",
    "   blackgram       1.00      1.00      1.00        16\n",
    "    chickpea       1.00      1.00      1.00        21\n",
    "     coconut       1.00      1.00      1.00        21\n",
    "      coffee       0.96      1.00      0.98        22\n",
    "      cotton       1.00      1.00      1.00        20\n",
    "      grapes       1.00      1.00      1.00        18\n",
    "        jute       1.00      0.93      0.96        28\n",
    " kidneybeans       1.00      1.00      1.00        14\n",
    "      lentil       0.96      1.00      0.98        23\n",
    "       maize       1.00      1.00      1.00        21\n",
    "       mango       1.00      1.00      1.00        26\n",
    "   mothbeans       1.00      0.95      0.97        19\n",
    "    mungbean       1.00      1.00      1.00        24\n",
    "   muskmelon       1.00      1.00      1.00        23\n",
    "      orange       1.00      1.00      1.00        29\n",
    "      papaya       1.00      1.00      1.00        19\n",
    "  pigeonpeas       1.00      1.00      1.00        18\n",
    " pomegranate       1.00      1.00      1.00        17\n",
    "        rice       0.94      1.00      0.97        16\n",
    "  watermelon       1.00      1.00      1.00        15\n",
    "\n",
    "    accuracy                           0.99       440\n",
    "   macro avg       0.99      0.99      0.99       440\n",
    "weighted avg       0.99      0.99      0.99       440\n",
    "\n",
    "# Cross validation score (XGBoost)\n",
    "score = cross_val_score(XB,features,target,cv=5)\n",
    "score\n",
    "[08:54:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[08:54:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[08:54:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[08:54:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "[08:54:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "array([0.99318182, 0.99318182, 0.99318182, 0.99090909, 0.99090909])\n",
    "Saving trained XGBoost model\n",
    "import pickle\n",
    "# Dump the trained Naive Bayes classifier with Pickle\n",
    "XB_pkl_filename = '../models/XGBoost.pkl'\n",
    "# Open the file to save as pkl file\n",
    "XB_Model_pkl = open(XB_pkl_filename, 'wb')\n",
    "pickle.dump(XB, XB_Model_pkl)\n",
    "# Close the pickle instances\n",
    "XB_Model_pkl.close()\n",
    "Accuracy Comparison\n",
    "plt.figure(figsize=[10,5],dpi = 100)\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Algorithm')\n",
    "sns.barplot(x = acc,y = model,palette='dark')\n",
    "\n",
    "accuracy_models = dict(zip(model, acc))\n",
    "for k, v in accuracy_models.items():\n",
    "    print (k, '-->', v)\n",
    "Decision Tree --> 0.9\n",
    "Naive Bayes --> 0.990909090909091\n",
    "SVM --> 0.9795454545454545\n",
    "Logistic Regression --> 0.9522727272727273\n",
    "RF --> 0.990909090909091\n",
    "XGBoost --> 0.9931818181818182\n",
    "Making a prediction\n",
    "data = np.array([[104,18, 30, 23.603016, 60.3, 6.7, 140.91]])\n",
    "prediction = RF.predict(data)\n",
    "print(prediction)\n",
    "['coffee']\n",
    "data = np.array([[83, 45, 60, 28, 70.3, 7.0, 150.9]])\n",
    "prediction = RF.predict(data)\n",
    "print(prediction)\n",
    "['jute']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
